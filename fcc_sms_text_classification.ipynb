{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "  !pip install tensorflow-datasets\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "jLWpgf6pRixW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will load the dataset, display basic information about its structure, show the first few rows of the training data, provide statistics about both datasets, show the distribution of labels, and display sample messages for both ham and spam categories."
      ],
      "metadata": {
        "id": "QnYAai8pRg5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_data = pd.read_csv(train_file_path, sep='\\t', names=['label', 'message'])\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t', names=['label', 'message'])\n",
        "\n",
        "# Display basic information about the datasets\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "print(train_data.head())\n",
        "\n",
        "# Display basic statistics of the datasets\n",
        "print(\"\\nTraining data info:\")\n",
        "train_data.info()\n",
        "\n",
        "print(\"\\nTest data info:\")\n",
        "test_data.info()\n",
        "\n",
        "# Display the distribution of labels in the training set\n",
        "print(\"\\nLabel distribution in training set:\")\n",
        "print(train_data['label'].value_counts(normalize=True))\n",
        "\n",
        "# Display some sample messages\n",
        "print(\"\\nSample ham message:\")\n",
        "print(train_data[train_data['label'] == 'ham']['message'].iloc[0])\n",
        "\n",
        "print(\"\\nSample spam message:\")\n",
        "print(train_data[train_data['label'] == 'spam']['message'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK punkt for tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing to both training and test data\n",
        "train_data['processed_message'] = train_data['message'].apply(preprocess_text)\n",
        "test_data['processed_message'] = test_data['message'].apply(preprocess_text)\n",
        "\n",
        "# Display a sample of preprocessed messages\n",
        "print(\"Sample preprocessed ham message:\")\n",
        "print(train_data[train_data['label'] == 'ham']['processed_message'].iloc[0])\n",
        "\n",
        "print(\"\\nSample preprocessed spam message:\")\n",
        "print(train_data[train_data['label'] == 'spam']['processed_message'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code accomplishes the following:\n",
        "\n",
        "1. Creates a vocabulary using Keras' Tokenizer\n",
        "2. Converts the preprocessed messages to sequences of word indices\n",
        "3. Pads the sequences to ensure uniform length\n",
        "4. Converts the labels to numerical values (0 for ham, 1 for spam)\n",
        "With these steps completed, our data is now ready for model creation and training. The next part will involve building and training the neural network model."
      ],
      "metadata": {
        "id": "NxvW_754Q_rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Create a vocabulary of unique words\n",
        "max_vocab_size = 10000  # You can adjust this value\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data['processed_message'])\n",
        "\n",
        "# Convert messages to sequences of word indices\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['processed_message'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['processed_message'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_length = 100  # You can adjust this value\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Convert labels to numerical values\n",
        "train_labels = (train_data['label'] == 'spam').astype(int)\n",
        "test_labels = (test_data['label'] == 'spam').astype(int)\n",
        "\n",
        "print(\"Vocabulary size:\", len(tokenizer.word_index))\n",
        "print(\"Shape of training data:\", train_padded.shape)\n",
        "print(\"Shape of test data:\", test_padded.shape)\n"
      ],
      "metadata": {
        "id": "hl42aw9sRAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Creation"
      ],
      "metadata": {
        "id": "2IaQHixF4HxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a neural network with the following architecture:\n",
        "\n",
        "An Embedding layer to learn word representations\n",
        "1. Two LSTM layers for sequence processing\n",
        "2. A Dense layer with ReLU activation\n",
        "3. A Dropout layer for regularization\n",
        "4. A final Dense layer with sigmoid activation for binary classification\n",
        "\n",
        "The model is then compiled using the Adam optimizer and binary cross-entropy loss function. We also include accuracy as a metric to monitor during training.\n",
        "\n",
        "This structure follows the approach outlined in the Approach.md file and should provide a good starting point for the SMS spam classification task. You can adjust the hyperparameters (such as embedding_dim, lstm_units, and the number of layers) to experiment with different model configurations."
      ],
      "metadata": {
        "id": "hZ0tucxP4K_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 16\n",
        "lstm_units = 32\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Pzz5fNwz4QFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Model Training"
      ],
      "metadata": {
        "id": "V4ZP5MeU5DfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements all five steps mentioned in the Model Training section:\n",
        "\n",
        "1. Splits the data into training and validation sets.\n",
        "2. Uses the already separated features (X) and labels (y).\n",
        "3. Trains the model using the preprocessed data.\n",
        "4. Monitors validation accuracy by plotting training and validation metrics.\n",
        "5. Provides a commented example of how to adjust hyperparameters.\n",
        "\n",
        "_Key Point:_ The class_weighting means that during training, the model gives twice as much importance to the spam class (labeled as 1) compared to the ham class (labeled as 0). This weighting scheme is particularly useful when dealing with imbalanced datasets, where spam messages might be less frequent than ham messages.\n",
        "\n",
        "After running this code, you'll have a trained model and visualizations of its performance over epochs. You can then analyze these results to determine if further hyperparameter tuning is necessary."
      ],
      "metadata": {
        "id": "ko5rEzDG5FuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_padded, train_labels, test_size=0.2, random_state=42)\n",
        "print(np.unique(y_train))\n",
        "print(np.unique(y_val))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "x_train_np = np.array(X_train)\n",
        "y_train_np = np.array(y_train)\n",
        "X_val_np = np.array(X_val)\n",
        "y_val_np = np.array(y_val)\n",
        "\n",
        "# Step 2: Features and labels are already separated in previous steps\n",
        "\n",
        "# Step 3: Train the model using the preprocessed data\n",
        "# Train the model with d class weights\n",
        "class_weights = {0: 1.0, 1: 2.0}  # 0 for ham, 1 for spam\n",
        "history = model.fit(\n",
        "    x=x_train_np,\n",
        "    y=y_train_np,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_np, y_val_np),\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Step 4: Monitor validation accuracy\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Adjust hyperparameters as needed\n",
        "# You can experiment with different values for epochs, batch_size, and learning rate\n",
        "# For example:\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n"
      ],
      "metadata": {
        "id": "oSGu_xwZ5NsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Implement predict_message Function"
      ],
      "metadata": {
        "id": "Tu8_cCSo6oKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation follows the three steps outlined in the Approach.md file:\n",
        "\n",
        "1. It preprocesses the input message using the same preprocess_text function used for training data.\n",
        "2. It uses the trained model to make a prediction on the preprocessed message.\n",
        "3. It returns the probability and corresponding label (ham or spam) based on the prediction.\n",
        "\n",
        "The function is then tested with a few example messages to demonstrate its functionality. You can run this code to see how the model performs on these examples and add more test cases as needed."
      ],
      "metadata": {
        "id": "3K-PPdqX6tU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_message(message):\n",
        "    # Step 1: Preprocess the input message\n",
        "    processed_message = preprocess_text(message)\n",
        "\n",
        "    # Step 2: Use the trained model to make a prediction\n",
        "    sequence = tokenizer.texts_to_sequences([processed_message])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "    prediction = model.predict(padded_sequence)[0][0]\n",
        "\n",
        "    # Step 3: Return the probability and corresponding label\n",
        "    label = \"spam\" if prediction > 0.5 else \"ham\"\n",
        "    return [float(prediction), label]\n",
        "\n",
        "# Test the function with a few examples\n",
        "examples = [\n",
        "    \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
        "    \"Hey, how are you doing? Want to meet up later?\",\n",
        "    \"WINNER!! As a valued network customer you have been selected to receive a £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n",
        "    \"Don't forget we have a meeting at 3pm today.\"\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    prediction = predict_message(example)\n",
        "    print(f\"Message: {example}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "c_RZzzFn6ylR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N4IktPJF6pB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      print(f\"Expected value: {ans}, but got {prediction[1]}\")\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}